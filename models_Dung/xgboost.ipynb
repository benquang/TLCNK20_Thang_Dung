{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkConf object with the desired configurations\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"xgboost\") \\\n",
    "    .setMaster(\"local[*]\")\n",
    "\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "conf.set(\"spark.executor.cores\", 4)\n",
    "conf.set(\"spark.dynamicAllocation.minExecutors\",\"2\")\n",
    "conf.set(\"spark.dynamicAllocation.maxExecutors\",\"10\")\n",
    "# Create a SparkContext using the SparkConf object\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "# Create a SparkSession using the SparkContext\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('xgboost') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+\n",
      "|Match_Date|        Home_Team|Home_Sh|Home_SoT|Home_Touches|Home_Tkl|Home_Int|Home_Blocks|  Home_xG_Expected|Home_npxG_Expected|Home_SCA_SCA|Home_GCA_SCA|Home_Cmp_Passes|Home_Att_Passes|Home_Cmp_percent_Passes|Home_PrgP_Passes|Home_Carries_Carries|Home_PrgC_Carries|Home_Att_Take_Ons|Home_Succ_Take_Ons|           Away_Team|Away_Sh|Away_SoT|Away_Touches|Away_Tkl|Away_Int|Away_Blocks|  Away_xG_Expected|Away_npxG_Expected|Away_SCA_SCA|Away_GCA_SCA|Away_Cmp_Passes|Away_Att_Passes|Away_Cmp_percent_Passes|Away_PrgP_Passes|Away_Carries_Carries|Away_PrgC_Carries|Away_Att_Take_Ons|Away_Succ_Take_Ons|Home_Score|Away_Score|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+\n",
      "|2023-05-28|          Chelsea|   13.5|     5.5|       636.5|    19.0|     6.0|       17.5|1.5499999999999998|1.5499999999999998|        24.5|         1.0|          463.0|          526.0|      87.55000000000001|            43.5|               371.5|             25.0|             26.5|              14.5|    Newcastle United|   15.0|     2.0|       801.0|     8.0|     4.0|        5.0|               1.2|               1.2|        26.0|         2.0|          661.0|          733.0|                   90.2|            36.0|               543.0|             28.0|             18.0|               7.0|         1|         1|\n",
      "|2023-05-28|        Brentford|   17.5|     6.5|       590.0|    15.5|     4.5|       18.5|              2.55|              2.55|        29.5|         5.0|          375.0|          478.0|                   78.2|            34.5|               324.0|             17.0|             14.5|               9.0|     Manchester City|   14.0|     6.5|       632.5|    15.5|     6.5|       11.0|               2.0|               2.0|        23.5|         2.0|          409.5|          524.5|                  77.95|            31.0|               298.0|             14.5|             17.5|               9.0|         1|         0|\n",
      "|2023-05-28|   Crystal Palace|   14.0|     5.0|       636.0|    17.0|     9.0|       12.5|               1.6|               1.6|        25.5|         3.5|          427.5|          530.0|                   79.9|            39.0|               375.5|             19.0|             24.0|              12.0|   Nottingham Forest|   11.0|     2.5|       654.0|    21.0|    10.0|       14.5|0.9500000000000001|0.9500000000000001|        20.5|         2.0|          447.5|          544.0|                   82.0|            33.0|               379.5|             14.0|             20.0|               9.5|         1|         1|\n",
      "|2023-05-28|Manchester United|   18.5|     6.5|       689.5|    22.5|     4.5|       12.0|               3.3|              2.95|        31.5|         3.5|          488.0|          580.5|                   84.0|            46.0|               393.5|             27.0|             24.5|              14.5|              Fulham|   15.0|     4.5|       708.0|    19.0|    11.5|       13.0|               1.2|               1.2|        25.5|         0.0|          497.0|          592.0|                   83.9|            46.0|               429.0|             21.0|             24.0|              11.5|         2|         1|\n",
      "|2023-05-28|          Arsenal|   12.5|     2.5|       667.5|    14.0|     6.5|        8.5|              0.75|              0.75|        19.0|         0.0|          478.5|          575.5|                  81.25|            50.0|               426.0|             22.0|             16.0|               6.5|Wolverhampton Wan...|   13.0|     4.0|       570.5|    17.5|     7.5|       12.0|               1.3|               1.3|        21.0|         1.5|          375.5|          473.5|                  78.75|            22.5|               332.0|             15.0|             22.0|              10.5|         5|         0|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read\n",
    "            .option(\"HEADER\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(\"./data/pl_matches_modified.csv\")\n",
    "           )\n",
    "df.show(5)\n",
    "\n",
    "# df['home_captain'] = df['home_captain'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onehot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/56585434/pyspark-pipeline-error-when-using-indexer-and-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder,StringIndexer,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unusedCols = ['home_fbrefMatchId ','away_fbrefMatchId']\n",
    "# outputCols = ['home_score','away_score']\n",
    "# inputCols = [column for column in df.columns if column not in outputCols and column not in unusedCols]\n",
    "# encodeCols = ['home_captain','away_captain','home_manager','away_manager','home_team','away_team','away_is_home_team']\n",
    "\n",
    "unusedCols = ['Match_Date']\n",
    "outputCols = ['Home_Score','Away_Score']\n",
    "inputCols = [column for column in df.columns if column not in outputCols and column not in unusedCols]\n",
    "encodeCols = ['Away_Team','Home_Team']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+\n",
      "|Match_Date|        Home_Team|Home_Sh|Home_SoT|Home_Touches|Home_Tkl|Home_Int|Home_Blocks|  Home_xG_Expected|Home_npxG_Expected|Home_SCA_SCA|Home_GCA_SCA|Home_Cmp_Passes|Home_Att_Passes|Home_Cmp_percent_Passes|Home_PrgP_Passes|Home_Carries_Carries|Home_PrgC_Carries|Home_Att_Take_Ons|Home_Succ_Take_Ons|           Away_Team|Away_Sh|Away_SoT|Away_Touches|Away_Tkl|Away_Int|Away_Blocks|  Away_xG_Expected|Away_npxG_Expected|Away_SCA_SCA|Away_GCA_SCA|Away_Cmp_Passes|Away_Att_Passes|Away_Cmp_percent_Passes|Away_PrgP_Passes|Away_Carries_Carries|Away_PrgC_Carries|Away_Att_Take_Ons|Away_Succ_Take_Ons|Home_Score|Away_Score|Away_Team_Index|Home_Team_Index|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+\n",
      "|2023-05-28|          Chelsea|   13.5|     5.5|       636.5|    19.0|     6.0|       17.5|1.5499999999999998|1.5499999999999998|        24.5|         1.0|          463.0|          526.0|      87.55000000000001|            43.5|               371.5|             25.0|             26.5|              14.5|    Newcastle United|   15.0|     2.0|       801.0|     8.0|     4.0|        5.0|               1.2|               1.2|        26.0|         2.0|          661.0|          733.0|                   90.2|            36.0|               543.0|             28.0|             18.0|               7.0|         1|         1|            9.0|            2.0|\n",
      "|2023-05-28|        Brentford|   17.5|     6.5|       590.0|    15.5|     4.5|       18.5|              2.55|              2.55|        29.5|         5.0|          375.0|          478.0|                   78.2|            34.5|               324.0|             17.0|             14.5|               9.0|     Manchester City|   14.0|     6.5|       632.5|    15.5|     6.5|       11.0|               2.0|               2.0|        23.5|         2.0|          409.5|          524.5|                  77.95|            31.0|               298.0|             14.5|             17.5|               9.0|         1|         0|            7.0|           20.0|\n",
      "|2023-05-28|   Crystal Palace|   14.0|     5.0|       636.0|    17.0|     9.0|       12.5|               1.6|               1.6|        25.5|         3.5|          427.5|          530.0|                   79.9|            39.0|               375.5|             19.0|             24.0|              12.0|   Nottingham Forest|   11.0|     2.5|       654.0|    21.0|    10.0|       14.5|0.9500000000000001|0.9500000000000001|        20.5|         2.0|          447.5|          544.0|                   82.0|            33.0|               379.5|             14.0|             20.0|               9.5|         1|         1|           25.0|            3.0|\n",
      "|2023-05-28|Manchester United|   18.5|     6.5|       689.5|    22.5|     4.5|       12.0|               3.3|              2.95|        31.5|         3.5|          488.0|          580.5|                   84.0|            46.0|               393.5|             27.0|             24.5|              14.5|              Fulham|   15.0|     4.5|       708.0|    19.0|    11.5|       13.0|               1.2|               1.2|        25.5|         0.0|          497.0|          592.0|                   83.9|            46.0|               429.0|             21.0|             24.0|              11.5|         2|         1|           17.0|            8.0|\n",
      "|2023-05-28|          Arsenal|   12.5|     2.5|       667.5|    14.0|     6.5|        8.5|              0.75|              0.75|        19.0|         0.0|          478.5|          575.5|                  81.25|            50.0|               426.0|             22.0|             16.0|               6.5|Wolverhampton Wan...|   13.0|     4.0|       570.5|    17.5|     7.5|       12.0|               1.3|               1.3|        21.0|         1.5|          375.5|          473.5|                  78.75|            22.5|               332.0|             15.0|             22.0|              10.5|         5|         0|           13.0|            0.0|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCols=encodeCols,outputCols = [encodeCol+\"_Index\" for encodeCol in encodeCols])\n",
    "indexerModel = indexer.fit(df)\n",
    "indexer_df = indexerModel.transform(df)\n",
    "indexer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+----------------+----------------+\n",
      "|Match_Date|        Home_Team|Home_Sh|Home_SoT|Home_Touches|Home_Tkl|Home_Int|Home_Blocks|  Home_xG_Expected|Home_npxG_Expected|Home_SCA_SCA|Home_GCA_SCA|Home_Cmp_Passes|Home_Att_Passes|Home_Cmp_percent_Passes|Home_PrgP_Passes|Home_Carries_Carries|Home_PrgC_Carries|Home_Att_Take_Ons|Home_Succ_Take_Ons|           Away_Team|Away_Sh|Away_SoT|Away_Touches|Away_Tkl|Away_Int|Away_Blocks|  Away_xG_Expected|Away_npxG_Expected|Away_SCA_SCA|Away_GCA_SCA|Away_Cmp_Passes|Away_Att_Passes|Away_Cmp_percent_Passes|Away_PrgP_Passes|Away_Carries_Carries|Away_PrgC_Carries|Away_Att_Take_Ons|Away_Succ_Take_Ons|Home_Score|Away_Score|Away_Team_Index|Home_Team_Index|Away_Team_Onehot|Home_Team_Onehot|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+----------------+----------------+\n",
      "|2023-05-28|          Chelsea|   13.5|     5.5|       636.5|    19.0|     6.0|       17.5|1.5499999999999998|1.5499999999999998|        24.5|         1.0|          463.0|          526.0|      87.55000000000001|            43.5|               371.5|             25.0|             26.5|              14.5|    Newcastle United|   15.0|     2.0|       801.0|     8.0|     4.0|        5.0|               1.2|               1.2|        26.0|         2.0|          661.0|          733.0|                   90.2|            36.0|               543.0|             28.0|             18.0|               7.0|         1|         1|            9.0|            2.0|  (26,[9],[1.0])|  (26,[2],[1.0])|\n",
      "|2023-05-28|        Brentford|   17.5|     6.5|       590.0|    15.5|     4.5|       18.5|              2.55|              2.55|        29.5|         5.0|          375.0|          478.0|                   78.2|            34.5|               324.0|             17.0|             14.5|               9.0|     Manchester City|   14.0|     6.5|       632.5|    15.5|     6.5|       11.0|               2.0|               2.0|        23.5|         2.0|          409.5|          524.5|                  77.95|            31.0|               298.0|             14.5|             17.5|               9.0|         1|         0|            7.0|           20.0|  (26,[7],[1.0])| (26,[20],[1.0])|\n",
      "|2023-05-28|   Crystal Palace|   14.0|     5.0|       636.0|    17.0|     9.0|       12.5|               1.6|               1.6|        25.5|         3.5|          427.5|          530.0|                   79.9|            39.0|               375.5|             19.0|             24.0|              12.0|   Nottingham Forest|   11.0|     2.5|       654.0|    21.0|    10.0|       14.5|0.9500000000000001|0.9500000000000001|        20.5|         2.0|          447.5|          544.0|                   82.0|            33.0|               379.5|             14.0|             20.0|               9.5|         1|         1|           25.0|            3.0| (26,[25],[1.0])|  (26,[3],[1.0])|\n",
      "|2023-05-28|Manchester United|   18.5|     6.5|       689.5|    22.5|     4.5|       12.0|               3.3|              2.95|        31.5|         3.5|          488.0|          580.5|                   84.0|            46.0|               393.5|             27.0|             24.5|              14.5|              Fulham|   15.0|     4.5|       708.0|    19.0|    11.5|       13.0|               1.2|               1.2|        25.5|         0.0|          497.0|          592.0|                   83.9|            46.0|               429.0|             21.0|             24.0|              11.5|         2|         1|           17.0|            8.0| (26,[17],[1.0])|  (26,[8],[1.0])|\n",
      "|2023-05-28|          Arsenal|   12.5|     2.5|       667.5|    14.0|     6.5|        8.5|              0.75|              0.75|        19.0|         0.0|          478.5|          575.5|                  81.25|            50.0|               426.0|             22.0|             16.0|               6.5|Wolverhampton Wan...|   13.0|     4.0|       570.5|    17.5|     7.5|       12.0|               1.3|               1.3|        21.0|         1.5|          375.5|          473.5|                  78.75|            22.5|               332.0|             15.0|             22.0|              10.5|         5|         0|           13.0|            0.0| (26,[13],[1.0])|  (26,[0],[1.0])|\n",
      "+----------+-----------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+--------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+---------------+---------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodeer = OneHotEncoder(inputCols=[encodeCol+\"_Index\" for encodeCol in encodeCols],outputCols=[encodeCol+\"_Onehot\" for encodeCol in encodeCols])\n",
    "encodeer_df = encodeer.fit(indexer_df).transform(indexer_df)\n",
    "encodeer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexedCols = [encodeCol+\"_Index\" for encodeCol in encodeCols]\n",
    "assembler_inputCols = encodeer_df.columns\n",
    "assembler_inputCols = [assembler_inputCol for assembler_inputCol in assembler_inputCols \\\n",
    "                       if assembler_inputCol not in encodeCols and assembler_inputCol not in indexedCols and assembler_inputCol not in unusedCols]\n",
    "assembler_df = encodeer_df.select(assembler_inputCols)\n",
    "assembler = VectorAssembler(inputCols=assembler_inputCols,outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Home_Sh',\n",
       " 'Home_SoT',\n",
       " 'Home_Touches',\n",
       " 'Home_Tkl',\n",
       " 'Home_Int',\n",
       " 'Home_Blocks',\n",
       " 'Home_xG_Expected',\n",
       " 'Home_npxG_Expected',\n",
       " 'Home_SCA_SCA',\n",
       " 'Home_GCA_SCA',\n",
       " 'Home_Cmp_Passes',\n",
       " 'Home_Att_Passes',\n",
       " 'Home_Cmp_percent_Passes',\n",
       " 'Home_PrgP_Passes',\n",
       " 'Home_Carries_Carries',\n",
       " 'Home_PrgC_Carries',\n",
       " 'Home_Att_Take_Ons',\n",
       " 'Home_Succ_Take_Ons',\n",
       " 'Away_Sh',\n",
       " 'Away_SoT',\n",
       " 'Away_Touches',\n",
       " 'Away_Tkl',\n",
       " 'Away_Int',\n",
       " 'Away_Blocks',\n",
       " 'Away_xG_Expected',\n",
       " 'Away_npxG_Expected',\n",
       " 'Away_SCA_SCA',\n",
       " 'Away_GCA_SCA',\n",
       " 'Away_Cmp_Passes',\n",
       " 'Away_Att_Passes',\n",
       " 'Away_Cmp_percent_Passes',\n",
       " 'Away_PrgP_Passes',\n",
       " 'Away_Carries_Carries',\n",
       " 'Away_PrgC_Carries',\n",
       " 'Away_Att_Take_Ons',\n",
       " 'Away_Succ_Take_Ons',\n",
       " 'Home_Score',\n",
       " 'Away_Score',\n",
       " 'Away_Team_Onehot',\n",
       " 'Home_Team_Onehot']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_inputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+----------------+----------------+--------------------+\n",
      "|Home_Sh|Home_SoT|Home_Touches|Home_Tkl|Home_Int|Home_Blocks|  Home_xG_Expected|Home_npxG_Expected|Home_SCA_SCA|Home_GCA_SCA|Home_Cmp_Passes|Home_Att_Passes|Home_Cmp_percent_Passes|Home_PrgP_Passes|Home_Carries_Carries|Home_PrgC_Carries|Home_Att_Take_Ons|Home_Succ_Take_Ons|Away_Sh|Away_SoT|Away_Touches|Away_Tkl|Away_Int|Away_Blocks|  Away_xG_Expected|Away_npxG_Expected|Away_SCA_SCA|Away_GCA_SCA|Away_Cmp_Passes|Away_Att_Passes|Away_Cmp_percent_Passes|Away_PrgP_Passes|Away_Carries_Carries|Away_PrgC_Carries|Away_Att_Take_Ons|Away_Succ_Take_Ons|Home_Score|Away_Score|Away_Team_Onehot|Home_Team_Onehot|            features|\n",
      "+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+----------------+----------------+--------------------+\n",
      "|   13.5|     5.5|       636.5|    19.0|     6.0|       17.5|1.5499999999999998|1.5499999999999998|        24.5|         1.0|          463.0|          526.0|      87.55000000000001|            43.5|               371.5|             25.0|             26.5|              14.5|   15.0|     2.0|       801.0|     8.0|     4.0|        5.0|               1.2|               1.2|        26.0|         2.0|          661.0|          733.0|                   90.2|            36.0|               543.0|             28.0|             18.0|               7.0|         1|         1|  (26,[9],[1.0])|  (26,[2],[1.0])|(90,[0,1,2,3,4,5,...|\n",
      "|   17.5|     6.5|       590.0|    15.5|     4.5|       18.5|              2.55|              2.55|        29.5|         5.0|          375.0|          478.0|                   78.2|            34.5|               324.0|             17.0|             14.5|               9.0|   14.0|     6.5|       632.5|    15.5|     6.5|       11.0|               2.0|               2.0|        23.5|         2.0|          409.5|          524.5|                  77.95|            31.0|               298.0|             14.5|             17.5|               9.0|         1|         0|  (26,[7],[1.0])| (26,[20],[1.0])|(90,[0,1,2,3,4,5,...|\n",
      "|   14.0|     5.0|       636.0|    17.0|     9.0|       12.5|               1.6|               1.6|        25.5|         3.5|          427.5|          530.0|                   79.9|            39.0|               375.5|             19.0|             24.0|              12.0|   11.0|     2.5|       654.0|    21.0|    10.0|       14.5|0.9500000000000001|0.9500000000000001|        20.5|         2.0|          447.5|          544.0|                   82.0|            33.0|               379.5|             14.0|             20.0|               9.5|         1|         1| (26,[25],[1.0])|  (26,[3],[1.0])|(90,[0,1,2,3,4,5,...|\n",
      "|   18.5|     6.5|       689.5|    22.5|     4.5|       12.0|               3.3|              2.95|        31.5|         3.5|          488.0|          580.5|                   84.0|            46.0|               393.5|             27.0|             24.5|              14.5|   15.0|     4.5|       708.0|    19.0|    11.5|       13.0|               1.2|               1.2|        25.5|         0.0|          497.0|          592.0|                   83.9|            46.0|               429.0|             21.0|             24.0|              11.5|         2|         1| (26,[17],[1.0])|  (26,[8],[1.0])|(90,[0,1,2,3,4,5,...|\n",
      "|   12.5|     2.5|       667.5|    14.0|     6.5|        8.5|              0.75|              0.75|        19.0|         0.0|          478.5|          575.5|                  81.25|            50.0|               426.0|             22.0|             16.0|               6.5|   13.0|     4.0|       570.5|    17.5|     7.5|       12.0|               1.3|               1.3|        21.0|         1.5|          375.5|          473.5|                  78.75|            22.5|               332.0|             15.0|             22.0|              10.5|         5|         0| (26,[13],[1.0])|  (26,[0],[1.0])|(90,[0,1,2,3,4,5,...|\n",
      "+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+-------+--------+------------+--------+--------+-----------+------------------+------------------+------------+------------+---------------+---------------+-----------------------+----------------+--------------------+-----------------+-----------------+------------------+----------+----------+----------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_vector = assembler.transform(assembler_df)\n",
    "output_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|            features|Home_Score|Away_Score|\n",
      "+--------------------+----------+----------+\n",
      "|(90,[0,1,2,3,4,5,...|         1|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         2|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         5|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         4|         4|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         2|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         4|\n",
      "|(90,[0,1,2,3,4,5,...|         2|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         4|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         0|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         3|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         3|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|         0|\n",
      "|(90,[0,1,2,3,4,5,...|         0|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         2|         2|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_vector.select(\"features\",\"Home_Score\",\"Away_Score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|Home_Score|\n",
      "+--------------------+----------+\n",
      "|(90,[0,1,2,3,4,5,...|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         1|\n",
      "|(90,[0,1,2,3,4,5,...|         2|\n",
      "|(90,[0,1,2,3,4,5,...|         5|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df = output_vector.select(\"features\",\"Home_Score\")\n",
    "# model_df = output_vector.select(\"Home_Blocks\",\"Home_Score\")\n",
    "model_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, test_df = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgBoost = SparkXGBRegressor(\n",
    "  features_col=\"features\",\n",
    "  label_col=\"Home_Score\",\n",
    "  prediction_col=\"Home_Score_Prediction\",\n",
    "  num_workers=1\n",
    ")\n",
    "\n",
    "# xgBoost = SparkXGBRegressor(\n",
    "#   features_col=\"Home_Blocks\",\n",
    "#   label_col=\"Home_Score\",\n",
    "#   prediction_col=\"Home_Score_Prediction\",\n",
    "#   num_workers=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(xgBoost.max_depth, [2, 5])\\\n",
    "  .addGrid(xgBoost.n_estimators, [10, 100])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SPARK_SUBMIT_OPTS'] = '--illegal-access=permit -Dio.netty.tryReflectionSetAccessible=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 18:43:41,949 INFO XGBoost-PySpark: _fit Running xgboost-2.0.0 on 1 workers with\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(19, 0) finished unsuccessfully.\njava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2216)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3042)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\Uni\\Nam4\\TieuLuan\\GitHub\\TLCNK20_Thang_Dung\\models_Dung\\xgboost.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Uni/Nam4/TieuLuan/GitHub/TLCNK20_Thang_Dung/models_Dung/xgboost.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m xgBoost_model \u001b[39m=\u001b[39m xgBoost\u001b[39m.\u001b[39;49mfit(training_df)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\spark\\core.py:1018\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m ret[\u001b[39m0\u001b[39m], ret[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1007\u001b[0m get_logger(\u001b[39m\"\u001b[39m\u001b[39mXGBoost-PySpark\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1008\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRunning xgboost-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m workers with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1009\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mbooster params: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     dmatrix_kwargs,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m (config, booster) \u001b[39m=\u001b[39m _run_job()\n\u001b[0;32m   1019\u001b[0m get_logger(\u001b[39m\"\u001b[39m\u001b[39mXGBoost-PySpark\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mFinished xgboost training!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1021\u001b[0m result_xgb_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_to_sklearn_model(\n\u001b[0;32m   1022\u001b[0m     \u001b[39mbytearray\u001b[39m(booster, \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m), config\n\u001b[0;32m   1023\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\spark\\core.py:1003\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit.<locals>._run_job\u001b[1;34m()\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_job\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    997\u001b[0m     ret \u001b[39m=\u001b[39m (\n\u001b[0;32m    998\u001b[0m         dataset\u001b[39m.\u001b[39;49mmapInPandas(\n\u001b[0;32m    999\u001b[0m             _train_booster, schema\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconfig string, booster string\u001b[39;49m\u001b[39m\"\u001b[39;49m  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   1000\u001b[0m         )\n\u001b[0;32m   1001\u001b[0m         \u001b[39m.\u001b[39;49mrdd\u001b[39m.\u001b[39;49mbarrier()\n\u001b[0;32m   1002\u001b[0m         \u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: x)\n\u001b[1;32m-> 1003\u001b[0m         \u001b[39m.\u001b[39;49mcollect()[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1004\u001b[0m     )\n\u001b[0;32m   1005\u001b[0m     \u001b[39mreturn\u001b[39;00m ret[\u001b[39m0\u001b[39m], ret[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(19, 0) finished unsuccessfully.\njava.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\r\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\r\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\r\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\r\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\r\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\r\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2216)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3042)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "xgBoost_model = xgBoost.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
